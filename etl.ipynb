{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be83d745-329f-4720-ae07-a0172c733913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "\n",
    "# Abstract Source Class\n",
    "class Source(ABC):\n",
    "    @abstractmethod\n",
    "    def get_data(self) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "# Abstract Destination Class\n",
    "class Destination(ABC):\n",
    "    @abstractmethod\n",
    "    def write_data(self, data: pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "# FileSource class that stores the path\n",
    "class FileSource(Source):\n",
    "    def __init__(self, path: str):\n",
    "        self.path = path\n",
    "\n",
    "# CSVFileSource class that reads all CSV files from a directory and stores them in a pandas DataFrame\n",
    "class CSVFileSource(FileSource):\n",
    "    def __init__(self, path: str):\n",
    "        super().__init__(path)\n",
    "        self.data = self._read_all_csv_files()\n",
    "\n",
    "    def _read_all_csv_files(self) -> pd.DataFrame:\n",
    "        all_files = [f for f in os.listdir(self.path) if f.endswith('.csv')]\n",
    "        dataframes = []\n",
    "        for file in all_files:\n",
    "            file_path = os.path.join(self.path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "        return pd.concat(dataframes, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    def get_data(self) -> pd.DataFrame:\n",
    "        return self.data\n",
    "\n",
    "\n",
    "# PostgresDestination class that writes pandas DataFrame to PostgreSQL using dictionary-based credentials\n",
    "class PostgresDestination(Destination):\n",
    "    def __init__(self, credentials: dict, destination_table: str):\n",
    "        super().__init__()\n",
    "        self.credentials = credentials\n",
    "        self.connection_string = self._create_connection_string()\n",
    "        self.destination_table = destination_table\n",
    "\n",
    "    def _create_connection_string(self) -> str:\n",
    "        user = self.credentials.get('user')\n",
    "        password = self.credentials.get('password')\n",
    "        host = self.credentials.get('host', 'localhost')\n",
    "        port = self.credentials.get('port', 5432)\n",
    "        database = self.credentials.get('database')\n",
    "        return f'postgresql://{user}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "    def write_data(self, data: pd.DataFrame):\n",
    "        engine = create_engine(self.connection_string)\n",
    "        data.to_sql(self.destination_table, engine, if_exists='replace', index=False)\n",
    "        print(\"Data written to PostgreSQL\")\n",
    "        engine.dispose()\n",
    "\n",
    "\n",
    "# Factory to create Source classes\n",
    "class SourceFactory:\n",
    "    @staticmethod\n",
    "    def create_source(source_type: str, *args, **kwargs) -> Source:\n",
    "        if source_type == \"csv\":\n",
    "            return CSVFileSource(*args, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown source type: {source_type}\")\n",
    "\n",
    "# Factory to create Destination classes\n",
    "class DestinationFactory:\n",
    "    @staticmethod\n",
    "    def create_destination(destination_type: str, *args, **kwargs) -> Destination:\n",
    "        if destination_type == \"postgres\":\n",
    "            return PostgresDestination(*args, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown destination type: {destination_type}\")\n",
    "\n",
    "# ETL class that manages the process\n",
    "class ETL:\n",
    "#    def __init__(self, source: Source, destination: Destination):\n",
    "    def __init__(self):\n",
    "        self.source = None\n",
    "        self.destination = None\n",
    "        self.configuration = get_configuration()\n",
    "        self._set_members()\n",
    "        \n",
    "        \n",
    "    def _set_members(self): \n",
    "        self.source=self.configuration[\"source\"][\"data\"]\n",
    "        self.source_type=self.configuration[\"source\"][\"type\"]\n",
    "        destination_credentials=self.configuration['destination']['credentials']\n",
    "        destination_type=self.configuration['destination']['type']\n",
    "        destination_table_name=self.configuration['destination']['destination_name']\n",
    "        self.destination = DestinationFactory.create_destination(destination_type, credentials=destination_credentials, destination_table=destination_table_name)\n",
    "        \n",
    "    def run(self):\n",
    "        source_data_dct={}\n",
    "        # get data\n",
    "        for k in self.source.keys():\n",
    "            source_data_dct[k]=SourceFactory.create_source(self.source_type, path=self.source[k]).get_data()\n",
    "        # table transformations\n",
    "        for tables_t in self.configuration[\"transformations\"]['tables']:\n",
    "            table_name=tables_t[\"table_name\"]\n",
    "            transformations=tables_t[\"transformations\"]\n",
    "            for tr in transformations:\n",
    "                source_data_dct[table_name]=eval(tr[\"name\"])(source_data_dct[table_name], tr[\"parameters\"])  \n",
    "        # joins\n",
    "        for join_t in self.configuration[\"transformations\"]['join']:\n",
    "            merged=source_data_dct[join_t[\"source_1\"]].merge(source_data_dct[join_t[\"source_2\"]], on=join_t[\"on\"], how=join_t[\"how\"]) \n",
    "        self.destination.write_data(merged)\n",
    "\n",
    "        \n",
    "def get_configuration(file_path = '/Users/amit/github/etl_task/config/conf.json'):\n",
    "    with open(file_path, 'r') as file:\n",
    "        conf = json.load(file)\n",
    "    return(conf)\n",
    "    \n",
    "\n",
    "def rename(df, parameters):\n",
    "    return(df.rename(columns=parameters))\n",
    "\n",
    "def set_types(df, parameters):\n",
    "    for k in parameters.keys():\n",
    "        if (parameters[k]==\"to_numeric\"):\n",
    "            df[k]=pd.to_numeric(df[k], errors='coerce')\n",
    "            df=df.dropna(subset=[k])\n",
    "        else:\n",
    "            df[k]=df[k].astype(parameters[k])\n",
    "    return(df)    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    etl = ETL()\n",
    "    etl.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbf79f-9125-4c53-8d32-fe874d328d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
